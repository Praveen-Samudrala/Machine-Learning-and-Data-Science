{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q4aD_Kn06qvw"
   },
   "source": [
    "# Disaster Tweets Classification Using Natural Language Processing (NLP)\n",
    "Twitter has become an important communication channel in times of emergency. The ubiquitousness of smartphones enables people to announce an emergency they’re observing in real-time. Because of this, more agencies are interested in programatically monitoring Twitter (i.e. disaster relief organizations and news agencies).\n",
    "\n",
    "But, it’s not always clear whether a person’s words are actually announcing a disaster.\n",
    "\n",
    "This dataset was created by the company figure-eight and originally shared on their ‘Data For Everyone’ website here.\n",
    "\n",
    "Tweet source: https://twitter.com/AnyOtherAnnaK/status/629195955506708480\n",
    "\n",
    "Competition link : https://www.kaggle.com/c/nlp-getting-started/overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UBDoT-a46wyX"
   },
   "source": [
    "## Problem Statement:\n",
    "To classify tweets whether they indicate a disaster or not. - Binary Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P5SD85iq7L-L"
   },
   "source": [
    "## Project Planning\n",
    "1. Import Libraries\n",
    "2. Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PC6fJtIr6Soe"
   },
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E0f4s3jB36eG",
    "outputId": "b8915c66-bf0c-4362-eb4d-5617f2a76e70"
   },
   "outputs": [],
   "source": [
    "#!pip install catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-18T01:38:15.853310Z",
     "iopub.status.busy": "2023-02-18T01:38:15.852920Z",
     "iopub.status.idle": "2023-02-18T01:38:33.152185Z",
     "shell.execute_reply": "2023-02-18T01:38:33.150983Z",
     "shell.execute_reply.started": "2023-02-18T01:38:15.853227Z"
    },
    "id": "jqQ8z-yy3Mt7",
    "outputId": "73a4e536-8053-43e5-8775-f77a929e6e51"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.set_option('display.max_colwidth', 150)\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import re\n",
    "import string\n",
    "from wordcloud import STOPWORDS\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk import FreqDist, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import bigrams\n",
    "import spacy\n",
    "from spacy.lang.en.examples import sentences \n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Machine Learning models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "#from catboost import CatBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "plt.rcParams.update({'font.size': 12})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hd9aNkMJ6X9f"
   },
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-18T02:14:57.419492Z",
     "iopub.status.busy": "2023-02-18T02:14:57.419065Z",
     "iopub.status.idle": "2023-02-18T02:14:57.494586Z",
     "shell.execute_reply": "2023-02-18T02:14:57.493542Z",
     "shell.execute_reply.started": "2023-02-18T02:14:57.419459Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "df_train = pd.read_csv(\"train.csv\")\n",
    "df_test = pd.read_csv(\"test.csv\")\n",
    "sub_sample = pd.read_csv(\"sample_submission.csv\")\n",
    "\n",
    "print (df_train.shape, df_test.shape, sub_sample.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_JJ1Hc-b3MyY"
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "#     for filename in filenames:\n",
    "#         print(os.path.join(dirname, filename))\n",
    "\n",
    "# df_train = pd.read_csv(r'/content/drive/MyDrive/Projects and Datasets/Disaster Tweet Classification NLP/train.csv')\n",
    "# df_test = pd.read_csv(r'/content/drive/MyDrive/Projects and Datasets/Disaster Tweet Classification NLP/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UxYH2QOL3M1I",
    "outputId": "f19f5e92-7eda-4655-bd93-03d841a17932"
   },
   "outputs": [],
   "source": [
    "print('df_train data shape: ',df_train.shape)\n",
    "print('df_test data shape: ',df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-18T02:15:35.198714Z",
     "iopub.status.busy": "2023-02-18T02:15:35.198354Z",
     "iopub.status.idle": "2023-02-18T02:15:35.219005Z",
     "shell.execute_reply": "2023-02-18T02:15:35.217463Z",
     "shell.execute_reply.started": "2023-02-18T02:15:35.198684Z"
    },
    "id": "HqM_Rjbs3M3Y",
    "outputId": "f886c421-11da-49f4-b721-fde8ab2480d7"
   },
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-18T02:15:37.601659Z",
     "iopub.status.busy": "2023-02-18T02:15:37.600951Z",
     "iopub.status.idle": "2023-02-18T02:15:37.611783Z",
     "shell.execute_reply": "2023-02-18T02:15:37.610298Z",
     "shell.execute_reply.started": "2023-02-18T02:15:37.601632Z"
    },
    "id": "y2rs5SLU6L0q",
    "outputId": "4297daab-a0ed-4e0f-b6a4-2f9cd6a2aceb"
   },
   "outputs": [],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iVgtDIes7e66"
   },
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-18T02:15:46.374052Z",
     "iopub.status.busy": "2023-02-18T02:15:46.373719Z",
     "iopub.status.idle": "2023-02-18T02:15:46.408992Z",
     "shell.execute_reply": "2023-02-18T02:15:46.407325Z",
     "shell.execute_reply.started": "2023-02-18T02:15:46.374028Z"
    },
    "id": "b7mRxS7G3M54",
    "outputId": "3535943e-f637-448f-f2f5-0da4beee0bd8"
   },
   "outputs": [],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-18T02:15:48.936301Z",
     "iopub.status.busy": "2023-02-18T02:15:48.935967Z",
     "iopub.status.idle": "2023-02-18T02:15:48.949780Z",
     "shell.execute_reply": "2023-02-18T02:15:48.948825Z",
     "shell.execute_reply.started": "2023-02-18T02:15:48.936277Z"
    },
    "id": "-R7KKvNa3M8H",
    "outputId": "7be00382-b62f-41af-8ca7-c334ec87995f"
   },
   "outputs": [],
   "source": [
    "print('Null values from df_train data')\n",
    "null_df_train = df_train.isnull().sum(axis=0)\n",
    "print(null_df_train)\n",
    "\n",
    "print('\\n\\nNull values from df_test data')\n",
    "null_df_test = df_test.isnull().sum(axis=0)\n",
    "print(null_df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-18T02:15:50.861659Z",
     "iopub.status.busy": "2023-02-18T02:15:50.861265Z",
     "iopub.status.idle": "2023-02-18T02:15:51.241808Z",
     "shell.execute_reply": "2023-02-18T02:15:51.240482Z",
     "shell.execute_reply.started": "2023-02-18T02:15:50.861626Z"
    },
    "id": "MPpHJoLbAIwy",
    "outputId": "02f9cf6d-0893-4722-c0fe-481ef5844b8b"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2, figsize=(10,5))\n",
    "\n",
    "sns.barplot(x = null_df_train.index, y = null_df_train.values/df_train.shape[0], ax=ax[0])\n",
    "sns.barplot(x = null_df_test.index, y = null_df_test.values/df_test.shape[0], ax=ax[1])\n",
    "\n",
    "ax[0].set_ylabel('Value Percentage', size=17)\n",
    "ax[0].set_title('Train Set', fontsize=17)\n",
    "ax[1].set_title('Test Set', fontsize=17)\n",
    "\n",
    "for ax in ax:\n",
    "  ax.tick_params(labelsize=10)\n",
    "  for p in ax.patches:\n",
    "      ax.annotate('{:.2f}'.format(p.get_height()),\n",
    "                  (p.get_x() + 0.4, p.get_height()),\n",
    "                  ha='center', va='bottom', color='black', size=17)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1xP6yzPhCZ1X"
   },
   "source": [
    "Very similar null value distribution of Train and Test data. It might indicate Train and test data are good samples from the population.\n",
    "\n",
    "Null values in 'Keyword' column is imputed with 'None' value first. As keyword is an important feature for summarizing the disaster, it can be filled with a word from tweet text. This treatment can be done during Preprocessing.  \n",
    "Need to explore 'location' field to impute null values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XYrShR4tGtaR"
   },
   "source": [
    "Lets explore the 'keyword' column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-18T02:15:56.971276Z",
     "iopub.status.busy": "2023-02-18T02:15:56.970906Z",
     "iopub.status.idle": "2023-02-18T02:15:57.160905Z",
     "shell.execute_reply": "2023-02-18T02:15:57.159036Z",
     "shell.execute_reply.started": "2023-02-18T02:15:56.971250Z"
    },
    "id": "DTuwYiGy3NDj",
    "outputId": "2ece1b08-e096-456a-b52a-18d67b057352"
   },
   "outputs": [],
   "source": [
    "# Proportion of Target Classes\n",
    "class_count = df_train.groupby('target').count()['id']/df_train.shape[0]\n",
    "print(class_count)\n",
    "\n",
    "plt.figure(figsize = (10,10))\n",
    "df_train.groupby('target').count()['id'].plot(kind='pie', \n",
    "                                          labels=['Not Disaster (57%)', 'Disaster (43%)'],\n",
    "                                          title='Target distribution in df_training Set',\n",
    "                                          ylabel='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-18T02:16:00.391020Z",
     "iopub.status.busy": "2023-02-18T02:16:00.390575Z",
     "iopub.status.idle": "2023-02-18T02:16:00.401425Z",
     "shell.execute_reply": "2023-02-18T02:16:00.400345Z",
     "shell.execute_reply.started": "2023-02-18T02:16:00.390984Z"
    },
    "id": "AqtbWGtM3M-1",
    "outputId": "44141076-d658-4d6e-9fd0-641bdd93d925"
   },
   "outputs": [],
   "source": [
    "df_train['keyword'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-18T02:16:05.379518Z",
     "iopub.status.busy": "2023-02-18T02:16:05.378515Z",
     "iopub.status.idle": "2023-02-18T02:16:05.389372Z",
     "shell.execute_reply": "2023-02-18T02:16:05.387616Z",
     "shell.execute_reply.started": "2023-02-18T02:16:05.379481Z"
    },
    "id": "viTsLYE-Eln1",
    "outputId": "e012854d-88ee-41f4-ec24-72d2182ccf37"
   },
   "outputs": [],
   "source": [
    "df_train_temp = df_train['keyword'].value_counts()\n",
    "df_train_temp[df_train_temp.values < 30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JuqHQ_aRE1r7"
   },
   "source": [
    "There is a '%20' character in the text, this needs to treated with space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "prT3cLZLFZon"
   },
   "source": [
    "### Treating 'keyword' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-18T02:16:09.073446Z",
     "iopub.status.busy": "2023-02-18T02:16:09.072817Z",
     "iopub.status.idle": "2023-02-18T02:16:09.098130Z",
     "shell.execute_reply": "2023-02-18T02:16:09.096658Z",
     "shell.execute_reply.started": "2023-02-18T02:16:09.073381Z"
    },
    "id": "UDstlKfzFPBt"
   },
   "outputs": [],
   "source": [
    "# Fill missing values with 'None'\n",
    "df_train['keyword'] = df_train['keyword'].fillna(f'None')\n",
    "df_test['keyword'] = df_test['keyword'].fillna(f'None')\n",
    "\n",
    "# fix '20%' typo in 'keyword' column\n",
    "df_train['keyword'] = df_train['keyword'].apply(lambda x: re.sub('%20', ' ', x))\n",
    "df_test['keyword'] = df_test['keyword'].apply(lambda x: re.sub('%20', ' ', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-18T02:16:11.885511Z",
     "iopub.status.busy": "2023-02-18T02:16:11.885164Z",
     "iopub.status.idle": "2023-02-18T02:16:12.152960Z",
     "shell.execute_reply": "2023-02-18T02:16:12.151848Z",
     "shell.execute_reply.started": "2023-02-18T02:16:11.885486Z"
    },
    "id": "-2tSxpDhGm11"
   },
   "outputs": [],
   "source": [
    "# Filling 'None' values in 'keyword' column with a word from 'keyword' column values, which is present in that text.\n",
    "# For each row with 'keyword' = None\n",
    "#   Check corresponding 'text' for an existing 'keyword' value\n",
    "#       If found, replace 'None' with that 'keyword' value\n",
    "no_keyword = df_train['keyword'] == 'None'\n",
    "keywords = np.unique(df_train[~no_keyword]['keyword'].to_numpy())\n",
    "\n",
    "for df in [df_train, df_test]:\n",
    "    for i in range(len(df)):\n",
    "        if df.loc[i, 'keyword'] == 'None':\n",
    "            for k in keywords:\n",
    "                if k in df.loc[i, 'text'].lower():\n",
    "                    df.loc[i, 'keyword'] = k\n",
    "                    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-18T02:16:14.114568Z",
     "iopub.status.busy": "2023-02-18T02:16:14.114184Z",
     "iopub.status.idle": "2023-02-18T02:16:14.130758Z",
     "shell.execute_reply": "2023-02-18T02:16:14.129431Z",
     "shell.execute_reply.started": "2023-02-18T02:16:14.114536Z"
    },
    "id": "773wa1GC3NBF",
    "outputId": "a0b2ca7d-a91f-49bf-a9c9-ed411f56e5bb"
   },
   "outputs": [],
   "source": [
    "print('Number of missing values left:')\n",
    "print('For Train set:', df_train[df_train['keyword'] == 'None'].shape[0])\n",
    "print('For Test set:', df_test[df_test['keyword'] == 'None'].shape[0])\n",
    "\n",
    "pd.concat([df_train[df_train['keyword'] == 'None']['text'], df_test[df_test['keyword'] == 'None']['text']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6EnXdXTL6kZb"
   },
   "source": [
    "These are the final 'text' columns values having 'None' value for 'keyword' column. They don't have any significant keyword, so left as they are, these rows have 'keyword'= None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-18T02:16:18.380289Z",
     "iopub.status.busy": "2023-02-18T02:16:18.379915Z",
     "iopub.status.idle": "2023-02-18T02:16:18.389559Z",
     "shell.execute_reply": "2023-02-18T02:16:18.388205Z",
     "shell.execute_reply.started": "2023-02-18T02:16:18.380260Z"
    },
    "id": "BMz4sv0sEk42"
   },
   "outputs": [],
   "source": [
    "# Fill missing values with 'None'\n",
    "df_train['location'] = df_train['location'].fillna(f'None')\n",
    "df_test['location'] = df_test['location'].fillna(f'None')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-18T02:16:19.807350Z",
     "iopub.status.busy": "2023-02-18T02:16:19.806993Z",
     "iopub.status.idle": "2023-02-18T02:16:19.825116Z",
     "shell.execute_reply": "2023-02-18T02:16:19.823559Z",
     "shell.execute_reply.started": "2023-02-18T02:16:19.807321Z"
    },
    "id": "OVtHqIowP1Wl",
    "outputId": "ef117a1d-7d06-4d08-91d8-655ff8431895"
   },
   "outputs": [],
   "source": [
    "df_train.isna().sum()\n",
    "df_test.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-18T02:16:22.518948Z",
     "iopub.status.busy": "2023-02-18T02:16:22.518552Z",
     "iopub.status.idle": "2023-02-18T02:16:22.856006Z",
     "shell.execute_reply": "2023-02-18T02:16:22.854725Z",
     "shell.execute_reply.started": "2023-02-18T02:16:22.518916Z"
    },
    "id": "Ga8UcFmF3NFw",
    "outputId": "a760578a-e470-4f8d-ea4b-bda10062932a"
   },
   "outputs": [],
   "source": [
    "# Top 20 keywords for each class\n",
    "\n",
    "disaster = df_train[df_train['target']==1]['keyword'].value_counts().head(20)\n",
    "non_disaster = df_train[df_train['target']==0]['keyword'].value_counts().head(20)\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize=(20,7))\n",
    "\n",
    "ax[0].set_title('Top keywords for disaster tweets')\n",
    "ax[0].set_xlabel('Count')\n",
    "sns.barplot(disaster, disaster.index, color='coral', ax=ax[0] )\n",
    "\n",
    "ax[1].set_title('Top keywords for non-disaster tweets')\n",
    "ax[1].set_xlabel('Count')\n",
    "sns.barplot(non_disaster, non_disaster.index, color='skyblue',  ax=ax[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y3QE_u7BDFLQ",
    "outputId": "90db1e14-c123-4082-b54a-0e7c2559df61"
   },
   "outputs": [],
   "source": [
    "# Tweet Length for both classes\n",
    "\n",
    "pos_tw_len = df_train[df_train['target'] == 1]['text'].str.len()\n",
    "neg_tw_len = df_train[df_train['target'] == 0]['text'].str.len()\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize=(20,7))\n",
    "ax[0].set_xlabel(' ')\n",
    "ax[0].set_title('Length of Disastrous Tweets')\n",
    "sns.distplot(pos_tw_len, label='Disaster Tweet length', ax=ax[0], color='red')\n",
    "\n",
    "ax[1].set_xlabel(' ')\n",
    "ax[1].set_title('Length of Non-Disastrous Tweets')\n",
    "sns.distplot(neg_tw_len, label='Non-Disaster Tweet length', ax=ax[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3Z4_fFlFDFN2",
    "outputId": "b582caf6-bb07-48d6-e788-ad96edd667ab"
   },
   "outputs": [],
   "source": [
    "# Word Count of Tweets in both classes\n",
    "pos_tw_len = df_train[df_train['target'] == 1]['text'].apply(lambda x: len(x.split(' ')))\n",
    "neg_tw_len = df_train[df_train['target'] == 0]['text'].apply(lambda x: len(x.split(' ')))\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize=(20,7))\n",
    "ax[0].set_xlabel(' ')\n",
    "ax[0].set_title('Word Count of Disastrous Tweets')\n",
    "sns.distplot(pos_tw_len, label='Disaster Tweet length', ax=ax[0], color='red')\n",
    "\n",
    "ax[1].set_xlabel(' ')\n",
    "ax[1].set_title('Word Count of Non-Disastrous Tweets')\n",
    "sns.distplot(neg_tw_len, label='Non-Disaster Tweet length', ax=ax[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tb3_1wluDFQr",
    "outputId": "b2078183-7a86-4e1f-894a-2302b94fc968"
   },
   "outputs": [],
   "source": [
    "# Number of Unique words in Tweets in both classes\n",
    "pos_tw_len = df_train[df_train['target'] == 1]['text'].apply(lambda x: len(set(x.split(' '))))\n",
    "neg_tw_len = df_train[df_train['target'] == 0]['text'].apply(lambda x: len(set(x.split(' '))))\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize=(20,7))\n",
    "ax[0].set_xlabel(' ')\n",
    "ax[0].set_title('Unique Word Count of Disastrous Tweets')\n",
    "sns.distplot(pos_tw_len, label='Disaster Tweet length', ax=ax[0], color='red')\n",
    "\n",
    "ax[1].set_xlabel(' ')\n",
    "ax[1].set_title('Unique Word Count of Non-Disastrous Tweets')\n",
    "sns.distplot(neg_tw_len, label='Non-Disaster Tweet length', ax=ax[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2rEkhCBPWuZr",
    "outputId": "7ca44572-0a64-43c8-810d-b7602f7a94d7"
   },
   "outputs": [],
   "source": [
    "# Number of occurances of # hashtag in a tweet in both classes\n",
    "pos_tw = df_train[df_train['target'] == 1]['text'].apply(lambda x: x.count('#'))\n",
    "neg_tw = df_train[df_train['target'] == 0]['text'].apply(lambda x: x.count('#'))\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize=(20,7))\n",
    "ax[0].set_xlabel(' ')\n",
    "ax[0].set_title('Hashtags Count of Disastrous Tweets')\n",
    "sns.distplot(pos_tw, label='Disaster Tweet length', ax=ax[0], color='red')\n",
    "\n",
    "ax[1].set_xlabel(' ')\n",
    "ax[1].set_title('Hashtags Count of Non-Disastrous Tweets')\n",
    "sns.distplot(neg_tw, label='Non-Disaster Tweet length', ax=ax[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fpvCpqLsRd7u",
    "outputId": "55e6d693-cddc-4b1b-c774-5abb916928c3"
   },
   "outputs": [],
   "source": [
    "# Top 20 Hastags for each class\n",
    "def find_hashtags(tweet):\n",
    "    return \" \".join([match.group(0)[1:] for match in re.finditer(r\"#\\w+\", tweet)]) or 'None'\n",
    "df_train['hashtags'] = df_train['text'].apply(lambda x: find_hashtags(x))\n",
    "df_test['hashtags'] = df_test['text'].apply(lambda x: find_hashtags(x))\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize=(20,7))\n",
    "\n",
    "\n",
    "freq_d = FreqDist(w for w in word_tokenize(' '.join(df_train.loc[df_train['target']==1, 'hashtags'])) if w != 'None')\n",
    "df_d = pd.DataFrame.from_dict(freq_d, orient='index', columns=['count'])\n",
    "hashtag_d = df_d.sort_values('count', ascending=False).head(20)\n",
    "sns.barplot(hashtag_d['count'], hashtag_d.index, color='coral', ax = ax[0])\n",
    "ax[0].set_title('Top 20 hastags in disaster tweets')\n",
    "\n",
    "freq_nd = FreqDist(w for w in word_tokenize(' '.join(df_train.loc[df_train['target']==0, 'hashtags'])) if w != 'None')\n",
    "df_nd = pd.DataFrame.from_dict(freq_nd, orient='index', columns=['count'])\n",
    "hashtag_nd = df_nd.sort_values('count', ascending=False).head(20)\n",
    "sns.barplot(hashtag_nd['count'], hashtag_nd.index, ax = ax[1], color='skyblue')\n",
    "ax[1].set_title('Top 20 hastags in non-disaster tweets')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O8uAA1iaxRWE"
   },
   "outputs": [],
   "source": [
    "# df_train[df_train['location'] != 'None']['location'].value_counts().plot(kind='pie')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s9d8Zx3PRFvj"
   },
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-18T02:17:23.165173Z",
     "iopub.status.busy": "2023-02-18T02:17:23.164788Z",
     "iopub.status.idle": "2023-02-18T02:17:23.174715Z",
     "shell.execute_reply": "2023-02-18T02:17:23.173106Z",
     "shell.execute_reply.started": "2023-02-18T02:17:23.165141Z"
    },
    "id": "2g7RLTYiWubz",
    "outputId": "32e290a1-13ce-47f8-9cb5-f1e8f934f3a4"
   },
   "outputs": [],
   "source": [
    "df_train['text'][0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CRgRmJzBRKop"
   },
   "source": [
    "### Data Cleaning\n",
    "Need RegExp to clean the text, remove puntuations, remove stop words, and Lemmatize words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-18T02:17:30.851805Z",
     "iopub.status.busy": "2023-02-18T02:17:30.851442Z",
     "iopub.status.idle": "2023-02-18T02:17:30.860752Z",
     "shell.execute_reply": "2023-02-18T02:17:30.859468Z",
     "shell.execute_reply.started": "2023-02-18T02:17:30.851776Z"
    },
    "id": "aG603ITtVLoa"
   },
   "outputs": [],
   "source": [
    "stop_words = set(list(STOPWORDS) + stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-18T02:17:43.644041Z",
     "iopub.status.busy": "2023-02-18T02:17:43.643673Z",
     "iopub.status.idle": "2023-02-18T02:17:43.652985Z",
     "shell.execute_reply": "2023-02-18T02:17:43.651640Z",
     "shell.execute_reply.started": "2023-02-18T02:17:43.644009Z"
    },
    "id": "QofWJr2CVLq1"
   },
   "outputs": [],
   "source": [
    "def preprocess(data):\n",
    "  '''The below preprocessing is performed.\n",
    "    1. Lower casing\n",
    "    2. Cleaning with RegExp\n",
    "    3. Tokenizing\n",
    "    4. Remove Punctuations\n",
    "    5. Remove Stopwords\n",
    "    6. Lemmatize\n",
    "  '''\n",
    "  # Converting all the text data to its lower form\n",
    "  data = data.lower()\n",
    "\n",
    "  # Cleaning with RegExp\n",
    "  # Removing URLs from the text data\n",
    "  data = re.sub(r'https?://\\S+|www\\.\\S+', '', data)\n",
    "  # Removing HTML Tags\n",
    "  data = re.sub(r\"<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});\", '', data)\n",
    "  #Removing Non-Ascii\n",
    "  data = re.sub(r'[^\\x00-\\x7f]','', data)\n",
    "  # Removing Emojis\n",
    "  emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "  data = emoji_pattern.sub(r'', data)\n",
    "\n",
    "  doc = nlp(data)\n",
    "\n",
    "  # Remove Punctuations\n",
    "  data = [token for token in doc if token.text not in string.punctuation]\n",
    "\n",
    "  # Remove stopwords\n",
    "  data = [token for token in data if not token.is_stop]\n",
    "\n",
    "  # Lemmatize\n",
    "  data = ' '.join([token.lemma_ for token in data])\n",
    "\n",
    "  return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-18T02:17:48.507592Z",
     "iopub.status.busy": "2023-02-18T02:17:48.507093Z",
     "iopub.status.idle": "2023-02-18T02:18:38.207483Z",
     "shell.execute_reply": "2023-02-18T02:18:38.205956Z",
     "shell.execute_reply.started": "2023-02-18T02:17:48.507551Z"
    },
    "id": "McpS5k8dsUGf",
    "outputId": "31c6ec6c-56d9-4dde-fc12-71ca823e7a08"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "df_train['cleaned_text'] = df_train['text'].apply(preprocess)\n",
    "df_train.head()\n",
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-18T02:18:56.739470Z",
     "iopub.status.busy": "2023-02-18T02:18:56.739049Z",
     "iopub.status.idle": "2023-02-18T02:19:17.833190Z",
     "shell.execute_reply": "2023-02-18T02:19:17.831662Z",
     "shell.execute_reply.started": "2023-02-18T02:18:56.739437Z"
    },
    "id": "tcdBu1QfsT6e",
    "outputId": "6f07d51c-499a-4e29-f462-149f3328f741"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "df_test['cleaned_text'] = df_test['text'].apply(preprocess)\n",
    "df_test.head()\n",
    "df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-18T02:22:09.879533Z",
     "iopub.status.busy": "2023-02-18T02:22:09.879116Z",
     "iopub.status.idle": "2023-02-18T02:22:09.885006Z",
     "shell.execute_reply": "2023-02-18T02:22:09.883320Z",
     "shell.execute_reply.started": "2023-02-18T02:22:09.879501Z"
    },
    "id": "GwFwANSoowU2"
   },
   "outputs": [],
   "source": [
    "# Dataset labels\n",
    "\n",
    "labels = df_train['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Aj9NPpxS4srx"
   },
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-18T02:37:36.023039Z",
     "iopub.status.busy": "2023-02-18T02:37:36.022673Z",
     "iopub.status.idle": "2023-02-18T02:37:36.029382Z",
     "shell.execute_reply": "2023-02-18T02:37:36.027897Z",
     "shell.execute_reply.started": "2023-02-18T02:37:36.023014Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MaxAbsScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-18T02:39:11.901752Z",
     "iopub.status.busy": "2023-02-18T02:39:11.900603Z",
     "iopub.status.idle": "2023-02-18T02:39:11.916075Z",
     "shell.execute_reply": "2023-02-18T02:39:11.914903Z",
     "shell.execute_reply.started": "2023-02-18T02:39:11.901716Z"
    },
    "id": "PsW4lGw76TbK"
   },
   "outputs": [],
   "source": [
    "scaler1 = MaxAbsScaler()\n",
    "train_bow_scaled = scaler1.fit_transform(train_bow)\n",
    "\n",
    "scaler2 = MaxAbsScaler()\n",
    "train_tfidf_scaled = scaler2.fit_transform(train_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-18T02:46:32.333459Z",
     "iopub.status.busy": "2023-02-18T02:46:32.333055Z",
     "iopub.status.idle": "2023-02-18T02:46:32.339093Z",
     "shell.execute_reply": "2023-02-18T02:46:32.337916Z",
     "shell.execute_reply.started": "2023-02-18T02:46:32.333425Z"
    },
    "id": "RUTgxT_Q6TSo"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-18T02:47:32.162880Z",
     "iopub.status.busy": "2023-02-18T02:47:32.162326Z",
     "iopub.status.idle": "2023-02-18T02:47:32.172757Z",
     "shell.execute_reply": "2023-02-18T02:47:32.171688Z",
     "shell.execute_reply.started": "2023-02-18T02:47:32.162832Z"
    }
   },
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-18T02:48:12.266590Z",
     "iopub.status.busy": "2023-02-18T02:48:12.265910Z",
     "iopub.status.idle": "2023-02-18T02:48:12.472366Z",
     "shell.execute_reply": "2023-02-18T02:48:12.471649Z",
     "shell.execute_reply.started": "2023-02-18T02:48:12.266558Z"
    },
    "id": "HfqyCXg06TX3"
   },
   "outputs": [],
   "source": [
    "logreg1 =  LogisticRegression(random_state=1)\n",
    "logreg1.fit(train_tfidf, labels)\n",
    "\n",
    "y_pred = logreg1.predict(train_tfidf_scaled)\n",
    "print('Accuracy: ', accuracy_score(labels, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SbTz9zWO4xVa"
   },
   "source": [
    "### Experiment 2\n",
    "Bag of Words with Array of ML Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-18T02:22:18.003741Z",
     "iopub.status.busy": "2023-02-18T02:22:18.002535Z",
     "iopub.status.idle": "2023-02-18T02:22:18.115037Z",
     "shell.execute_reply": "2023-02-18T02:22:18.113628Z",
     "shell.execute_reply.started": "2023-02-18T02:22:18.003680Z"
    },
    "id": "KTX8-QA5VLtd",
    "outputId": "cbc76783-1490-4bf7-a901-d9e990e4b394"
   },
   "outputs": [],
   "source": [
    "# Bag-of-Words Model\n",
    "bow = CountVectorizer()\n",
    "train_bow = bow.fit_transform(df_train['cleaned_text'])\n",
    "\n",
    "print('Vocabulary Length : ', len(bow.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W4YgFyE-7slw",
    "outputId": "fb57a3c5-3a14-4109-bfd4-cee6232fd52a"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-18T02:30:23.330475Z",
     "iopub.status.busy": "2023-02-18T02:30:23.330040Z",
     "iopub.status.idle": "2023-02-18T02:30:23.341793Z",
     "shell.execute_reply": "2023-02-18T02:30:23.340208Z",
     "shell.execute_reply.started": "2023-02-18T02:30:23.330443Z"
    },
    "id": "nHrQmAQsreHY"
   },
   "outputs": [],
   "source": [
    "# Model Experimentations\n",
    "# Machine Learning models\n",
    "\n",
    "# pipe_lr = make_pipeline(MinMaxScaler(), LogisticRegression(random_state=1)) \n",
    "# pipe_svm = make_pipeline(MinMaxScaler(), SVC(random_state=1))\n",
    "\n",
    "\n",
    "model = {'Logistic Regression' : LogisticRegression(random_state=1),\n",
    "         'Support Vector Machines' : SVC(random_state=1),\n",
    "         'Multinomial Naive Bayes' : MultinomialNB(),\n",
    "         'Decision Trees' : DecisionTreeClassifier(random_state=1),\n",
    "         'Random Forest Classifier' : RandomForestClassifier(random_state=1),\n",
    "         'lightGBM': LGBMClassifier(random_state=1),\n",
    "         'XG Boosting' : XGBClassifier(random_state=1)}\n",
    "\n",
    "\n",
    "def _model_experimentation_pipeline(X, Y, models):\n",
    "    model_score = {}\n",
    "    for name, model in models.items():\n",
    "        model_ = model\n",
    "        print(\"5-Fold Cross-Validation : \", name)\n",
    "        \n",
    "        model_score[name] = np.mean(cross_val_score(model_,X, Y,\n",
    "                                              cv=5,\n",
    "                                              scoring='accuracy',\n",
    "                                              verbose=2,\n",
    "                                              n_jobs=-1))\n",
    "        \n",
    "    # Converting model_score to DataFrame\n",
    "    model_score = {'5-Fold CV Score': model_score}\n",
    "    model_score_df = pd.DataFrame(model_score)\n",
    "    model_score_df.rename_axis('Model', inplace=True)\n",
    "    model_score_df.reset_index(inplace=True)\n",
    "    model_score_df.sort_values('5-Fold CV Score', ascending=False, inplace=True)\n",
    "    return model_score_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sUimJJaRAypa"
   },
   "outputs": [],
   "source": [
    "# GridSearch\n",
    "# pipe_lr = make_pipeline(MinMaxScaler(), LogisticRegression(random_state=1, max_iter=1000)\n",
    "# logreg = GridSearchCV(estimator=pipe_lr, \n",
    "#                       param_grid=lr_param_grid,\n",
    "#                       scoring='accuracy', cv=5)\n",
    "\n",
    "# grid_arr = [logreg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-18T02:30:35.281706Z",
     "iopub.status.busy": "2023-02-18T02:30:35.281312Z",
     "iopub.status.idle": "2023-02-18T02:31:29.226765Z",
     "shell.execute_reply": "2023-02-18T02:31:29.225643Z",
     "shell.execute_reply.started": "2023-02-18T02:30:35.281674Z"
    },
    "id": "uuS4a66m6TfG",
    "outputId": "721b4dbc-5e6d-4f5b-dd88-734a8ea2aaae"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "bow_score = _model_experimentation_pipeline(train_bow, labels, model)\n",
    "bow_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KKJ7WOpH48V0"
   },
   "source": [
    "### Experiment 3\n",
    "TF-IDF Vectorization with Array of ML models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-18T02:34:03.222096Z",
     "iopub.status.busy": "2023-02-18T02:34:03.221729Z",
     "iopub.status.idle": "2023-02-18T02:34:03.330626Z",
     "shell.execute_reply": "2023-02-18T02:34:03.329317Z",
     "shell.execute_reply.started": "2023-02-18T02:34:03.222065Z"
    },
    "id": "CIXT5xp1VLwA"
   },
   "outputs": [],
   "source": [
    "# TF-IDF Model\n",
    "tfidf = TfidfVectorizer()\n",
    "train_tfidf = tfidf.fit_transform(df_train['cleaned_text'])\n",
    "print('Vocabulary Length : ', len(tfidf.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-18T02:34:41.611510Z",
     "iopub.status.busy": "2023-02-18T02:34:41.611054Z",
     "iopub.status.idle": "2023-02-18T02:35:44.175204Z",
     "shell.execute_reply": "2023-02-18T02:35:44.174192Z",
     "shell.execute_reply.started": "2023-02-18T02:34:41.611475Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "tfidf_score = _model_experimentation_pipeline(train_tfidf, labels, model)\n",
    "tfidf_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "f51647a10789571719965e586262b0acd4dd9d43844e501acfa287020e3b2261"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
