{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<h2> <div align=\"center\">Development of Dynamic Machine Model based on Static model to detect Network Intrusion on real-time datastream using Kafka Integration</div> </h2>\n",
        "\n",
        "---\n",
        "---"
      ],
      "metadata": {
        "id": "5tdN-7P3FCZM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "48B9eAMMhAgw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d06e083-1625-41fa-eb65-ca0b678ea08c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow-io in c:\\softwares\\anaconda\\envs\\tfgpu\\lib\\site-packages (0.27.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem==0.27.0 in c:\\softwares\\anaconda\\envs\\tfgpu\\lib\\site-packages (from tensorflow-io) (0.27.0)\n",
            "Requirement already satisfied: kafka-python in c:\\softwares\\anaconda\\envs\\tfgpu\\lib\\site-packages (2.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow-io\n",
        "!pip install kafka-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m6KXZuTBWgRm"
      },
      "outputs": [],
      "source": [
        "# Necessary modules\n",
        "import random\n",
        "random.seed(22)\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "from datetime import datetime\n",
        "import time\n",
        "import threading\n",
        "import json\n",
        "from kafka import KafkaProducer, KafkaConsumer\n",
        "from kafka.errors import KafkaError\n",
        "import pickle \n",
        "\n",
        "# Visualization\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "sns.set_style('whitegrid')\n",
        "sns.set_context('notebook')\n",
        "matplotlib.rcParams['figure.figsize'] = (12,8)\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.feature_extraction import FeatureHasher\n",
        "from sklearn.feature_selection import mutual_info_classif, VarianceThreshold\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.pipeline import make_pipeline\n",
        "import xgboost as xgb\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating Kafka Consumer\n",
        "consumer = KafkaConsumer('ml-raw-dns',\n",
        "                         bootstrap_servers=\"localhost:9092\",\n",
        "                         auto_offset_reset='earliest',\n",
        "                         enable_auto_commit=False)"
      ],
      "metadata": {
        "id": "JNsHYDdBxkAb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "static_win_acc = []\n",
        "dynamic_win_acc = []\n",
        "static_cm = []\n",
        "dynamic_cm = []\n",
        "df = pd.DataFrame(columns=['timestamp','FQDN_count','subdomain_length','upper','lower',\n",
        "                           'numeric','entropy','special','labels','labels_max','labels_average',\n",
        "                           'longest_word','sld','len','subdomain','Target Attack'])\n",
        "for i in range(1,16):\n",
        "  dataStream=[]\n",
        "  realtimeData=[]\n",
        "  df.drop(df.index, inplace=True)\n",
        "  ranges=0\n",
        "  for m in consumer:\n",
        "      if(ranges>((i*1000)+50) and ranges <= ((i+1)*1000)+50):\n",
        "          dataStream.append(m)\n",
        "      if(ranges==((i+1)*1000)+50):\n",
        "          break\n",
        "      ranges=ranges+1\n",
        "  for j in range(0,1000):\n",
        "      rec_string=(dataStream[j][6]).decode(\"utf-8\")\n",
        "      rec_string=rec_string[2:-3]\n",
        "      rec_string=rec_string.split(\",\")\n",
        "      realtimeData.append(rec_string)\n",
        "  df=pd.DataFrame(columns=['timestamp','FQDN_count','subdomain_length','upper','lower',\n",
        "                            'numeric','entropy','special','labels','labels_max','labels_average',\n",
        "                            'longest_word','sld','len','subdomain','Target Attack'])\n",
        "  for loc in range(0,len(realtimeData)):\n",
        "      df = df.append(pd.Series(realtimeData[loc], index=['timestamp','FQDN_count','subdomain_length','upper','lower','numeric',\n",
        "                                                          'entropy','special','labels','labels_max','labels_average','longest_word',\n",
        "                                                          'sld','len','subdomain','Target Attack']),ignore_index=True)\n",
        "  hash = FeatureHasher(n_features=5, input_type='string')\n",
        "  df.drop(['timestamp'], axis=1, inplace=True) \n",
        "  df = df.replace(np.nan, 0)\n",
        "  longest_word = hash.fit_transform(df[['longest_word']].astype(str).values).todense()\n",
        "  longest_word = pd.DataFrame(longest_word, columns=['longest_word_hash'+str(i) for i in range(1,6)])\n",
        "  sld  = hash.fit_transform(df[['sld']].astype(str).values).todense()\n",
        "  sld = pd.DataFrame(sld, columns=['sld'+str(i) for i in range(1,6)])\n",
        "  df.drop(['longest_word', 'sld'],axis=1,inplace=True)\n",
        "  df = pd.concat([df,longest_word,sld],axis=1)\n",
        "  X = df.drop(['Target Attack'], axis = 1)\n",
        "  y = df['Target Attack'].astype(float)\n",
        "  X_new = X[['FQDN_count', 'subdomain_length', 'lower', 'numeric', 'entropy', 'special',\n",
        "            'labels', 'labels_max', 'labels_average', 'len', 'subdomain',\n",
        "            'longest_word_hash2', 'longest_word_hash5', 'sld2']].astype(float)\n",
        "  \n",
        "  X_train, X_test, y_train, y_test = train_test_split(X_new, y, test_size=0.25, random_state=22)\n",
        "  \n",
        "\n",
        "  \n",
        "  statModel = pickle.load(open('XGB_Model.pkl', 'rb'))\n",
        "  statPred = statModel.predict(X_test)\n",
        "  statAcc = statModel.score(X_test, y_test)\n",
        "  static_win_acc.append(statAcc)\n",
        "  static_cm.append((statPred))\n",
        "\n",
        "  pipe_xgbc = make_pipeline(XGBClassifier(verbosity=1,random_state=22,n_jobs =-1))\n",
        "  param_grid_xgbc = {'xgbclassifier__n_estimators': [1000],'xgbclassifier__reg_alpha': [0.01],'xgbclassifier__gamma': [0.001],'xgbclassifier__learning_rate': [0.1], 'xgbclassifier__scale_pos_weight': [0.9],'xgbclassifier__subsample': [0.9], 'xgbclassifier__min_child_weight':[1]}\n",
        "  dynaModel = GridSearchCV(pipe_xgbc, param_grid_xgbc, cv=5, n_jobs=-1, scoring=\"f1\" )\n",
        "  dynaModel.fit(X_train, y_train)\n",
        "  dynaPred = dynaModel.predict(X_test)\n",
        "  dynaModel_result=dynaModel.score(X_test,y_test)\n",
        "  dynamic_win_acc.append(dynaModel_result) \n",
        "  dynamic_cm.append((dynaPred))\n",
        "\n",
        "print('Static Model Accuracies',static_win_acc)\n",
        "print('Dynamic Model Accuracies',dynamic_win_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KQ4MMwyvxslG",
        "outputId": "3680bb6c-a89f-470b-c3fa-2e89d7c7f8dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Static Model Accuracies [0.8502994011976048, 0.8317460317460318, 0.8753993610223643, 0.8698224852071006, 0.8417910447761193, 0.8721311475409836, 0.9054441260744985, 0.8498402555910544, 0.8145695364238411, 0.8613569321533924, 0.8463949843260188, 0.8433734939759037, 0.8690095846645367, 0.8387096774193548, 0.8708708708708708]\n",
            "Dynamic Model Accuracies [0.8328267477203647, 0.8242811501597446, 0.8645161290322582, 0.8656716417910447, 0.8468468468468469, 0.8675496688741721, 0.8959537572254335, 0.8414239482200647, 0.8039867109634551, 0.8511904761904762, 0.8444444444444444, 0.844984802431611, 0.8717948717948718, 0.814569536423841, 0.8606060606060607]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "staticAccuracies = {}\n",
        "staticAccuracies_max = []\n",
        "windows = ['win_'+str(i) for i in range(1,16)]\n",
        "for a,b in zip(list(windows), static_win_acc):\n",
        "    staticAccuracies[a] = b\n",
        "print(staticAccuracies)\n",
        "print('Max Acc Window:', max(staticAccuracies, key=staticAccuracies.get))\n",
        "print('Max Acc: ',staticAccuracies[max(staticAccuracies, key=staticAccuracies.get)])\n",
        "staticAccuracies_max.append(staticAccuracies[max(staticAccuracies, key=staticAccuracies.get)])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0aEZ0mrH1QS3",
        "outputId": "cd0bf2bc-813c-4974-97a8-7f97c42333cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'win_1': 0.8502994011976048, 'win_2': 0.8317460317460318, 'win_3': 0.8753993610223643, 'win_4': 0.8698224852071006, 'win_5': 0.8417910447761193, 'win_6': 0.8721311475409836, 'win_7': 0.9054441260744985, 'win_8': 0.8498402555910544, 'win_9': 0.8145695364238411, 'win_10': 0.8613569321533924, 'win_11': 0.8463949843260188, 'win_12': 0.8433734939759037, 'win_13': 0.8690095846645367, 'win_14': 0.8387096774193548, 'win_15': 0.8708708708708708}\n",
            "Max Acc Window: win_7\n",
            "Max Acc:  0.9054441260744985\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dynamicAccuracies = {}\n",
        "dynamicAccuracies_max = []\n",
        "for a,b in zip(list(windows), dynamic_win_acc):\n",
        "    dynamicAccuracies[a] = b\n",
        "print(dynamicAccuracies)\n",
        "print('Max Acc Window: ',max(dynamicAccuracies, key=dynamicAccuracies.get))\n",
        "print('Max Acc: ',dynamicAccuracies[max(dynamicAccuracies, key=dynamicAccuracies.get)])\n",
        "dynamicAccuracies_max.append(dynamicAccuracies[max(dynamicAccuracies, key=dynamicAccuracies.get)])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oUQbmwPT1UE-",
        "outputId": "8e4da065-59cb-4041-cd0c-a775d8f07e8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'win_1': 0.8328267477203647, 'win_2': 0.8242811501597446, 'win_3': 0.8645161290322582, 'win_4': 0.8656716417910447, 'win_5': 0.8468468468468469, 'win_6': 0.8675496688741721, 'win_7': 0.8959537572254335, 'win_8': 0.8414239482200647, 'win_9': 0.8039867109634551, 'win_10': 0.8511904761904762, 'win_11': 0.8444444444444444, 'win_12': 0.844984802431611, 'win_13': 0.8717948717948718, 'win_14': 0.814569536423841, 'win_15': 0.8606060606060607}\n",
            "Max Acc Window:  win_7\n",
            "Max Acc:  0.8959537572254335\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fZijEX5eweEp",
        "outputId": "9b254d33-5a58-4adc-9147-4b025e050760",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion matrix of Static Model\n",
            "\n",
            " [[ 23  82]\n",
            " [ 36 109]]\n",
            "Confusion matrix of Dynamic Model\n",
            "\n",
            " [[ 27  78]\n",
            " [ 37 108]]\n"
          ]
        }
      ],
      "source": [
        "#Static and Dynamic model Confusion Matrix\n",
        "static_cm = confusion_matrix(y_test, static_cm[staticAccuracies_max.index(max(staticAccuracies_max))])\n",
        "print('Confusion matrix of Static Model\\n\\n', static_cm)\n",
        "dynamic_cm = confusion_matrix(y_test, dynamic_cm[dynamicAccuracies_max.index(max(dynamicAccuracies_max))])\n",
        "print('Confusion matrix of Dynamic Model\\n\\n', dynamic_cm)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SKFf73i9weEr"
      },
      "outputs": [],
      "source": [
        "#Static and Dynamic model Classification Report\n",
        "print('Classification Report of Static Model\\n')\n",
        "print(classification_report(y_test, static_cm[staticAccuracies_max.index(max(staticAccuracies_max))]))\n",
        "print('\\Classification Report of Dynamic Model\\n')\n",
        "print(classification_report(y_test, dynamic_cm[dynamicAccuracies_max.index(max(dynamicAccuracies_max))]))"
      ]
    }
  ]
}